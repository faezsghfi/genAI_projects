# -*- coding: utf-8 -*-
"""Untitled63.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBb293kjWnih15y1xXi3I2W2U_Q5DVG3
"""

!pip install transformers
!pip install torch torchvision
!pip install pillow

"""### Captioning"""

from transformers import BlipProcessor, BlipForConditionalGeneration
import torch
from PIL import Image
import requests

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Ø¢Ø¯Ø±Ø³ ÛŒÚ© Ø¹Ú©Ø³ ÙˆØ§Ù‚Ø¹ÛŒ
img_url = "https://images.unsplash.com/photo-1493612276216-ee3925520721?fm=jpg&q=60&w=3000&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8cmFuZG9tfGVufDB8fDB8fHww"
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ùˆ ØªØ¨Ø¯ÛŒÙ„Ø´ Ø¨Ù‡ RGB
image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')
image

# ØªØ¨Ø¯ÛŒÙ„ ØªØµÙˆÛŒØ± Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„
inputs = processor(image, return_tensors="pt")

# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ
out = model.generate(**inputs)

# ØªØ±Ø¬Ù…Ù‡â€ŒÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù†
caption = processor.decode(out[0], skip_special_tokens=True)

print("ğŸ“· Ú©Ù¾Ø´Ù† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:", caption)

"""### VQA"""

from transformers import BlipForQuestionAnswering, BlipProcessor

# Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡
processor2 = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
model2 = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")

# Ù„ÙˆØ¯ ÛŒÚ© ØªØµÙˆÛŒØ±
# img_url = "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg"
img_url = "https://images.unsplash.com/photo-1493612276216-ee3925520721?fm=jpg&q=60&w=3000&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8cmFuZG9tfGVufDB8fDB8fHww"
image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

# Ø³ÙˆØ§Ù„ Ù…ØªÙ†ÛŒ
# question = "What is the girl doing?"
question = "What color is the dress's fabric?"
image

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
inputs = processor2(image, question, return_tensors="pt")

# ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
out = model2.generate(**inputs)

# Ø¯ÛŒÚ©ÙˆØ¯ÛŒÙ†Ú¯ Ù¾Ø§Ø³Ø®
answer = processor2.decode(out[0], skip_special_tokens=True)
print("ğŸ¤” Ù¾Ø§Ø³Ø®:", answer)